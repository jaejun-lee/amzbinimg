'''
Train a CNN on the CIFAR-10 small images dataset. Baseline code is from Keras
examples Github repository. My primary use for this script is to test an
AWS AMI that I am using/creating for the project. Additionally, I have made
modifications to attain improvements over original performance and gain a better
understanding of the Keras framework and TensorFlow.

Keras claimed 75% validation accuracy in 25 epochs, and 79% after 50 epochs in
the baseline code. However, I encountered a run error. The fit_generator method
required the steps_per_epoch parameter to be defined. Defining with a value of 1
allowed the script to run but the test accuuracy was only around 0.1. Changing
steps_per_epoch = x_train.shape[0]/batch_size and setting batch size to 100
got me up to 0.7854 in 25 epochs. This will be my new baseline.
'''

from __future__ import print_function
import keras
from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D
import os
import matplotlib.pyplot as plt
import datetime as dt

def get_model1(x_trn, num_classes):
    model = Sequential()
    model.add(Conv2D(32, (3, 3), padding='same',
                     input_shape=x_trn.shape[1:]))
    model.add(Activation('relu'))
    model.add(Conv2D(32, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Conv2D(64, (3, 3), padding='same'))
    model.add(Activation('relu'))
    model.add(Conv2D(64, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes))
    model.add(Activation('softmax'))

    # initiate RMSprop optimizer
    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)

    # Let's train the model using RMSprop
    model.compile(loss='categorical_crossentropy',
                  optimizer=opt,
                  metrics=['accuracy'])
    return model


def main():
    # The data, shuffled and split between train and test sets:
    (x_train, y_train), (x_test, y_test) = cifar10.load_data()
    print('x_train shape:', x_train.shape)
    print(x_train.shape[0], 'train samples')    # 50k
    print(x_test.shape[0], 'test samples')  # 10k
    print(x_train.shape[1:], 'input shape') # 32x32x3

    # # Convert class vectors to binary class matrices.
    num_classes = 10
    y_train = keras.utils.to_categorical(y_train, num_classes)
    y_test = keras.utils.to_categorical(y_test, num_classes)

    # obtain a predefined model for test
    model = get_model1(x_train, num_classes)

    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train /= 255
    x_test /= 255

    # there are 50000 training samples and 10000 test samples in the current
    # CIFAR-10 data set (12/22/2017).
    # for fit_generator, batch_size * steps_per_epoch = training data size
    batch_sizes = []
    scores = []
    start_time = dt.datetime.now()
    for x in range(20, 600, 20):    #15206s
        batch_size = x
        epochs = 25
        steps_per_epoch = x_train.shape[0]/batch_size
        data_augmentation = True

        if not data_augmentation:
            print('Not using data augmentation.')
            model.fit(x_train, y_train,
                      batch_size=batch_size,
                      epochs=epochs,
                      validation_data=(x_test, y_test),
                      shuffle=True)
        else:
            print('Using real-time data augmentation.')
            # This will do preprocessing and realtime data augmentation:
            datagen = ImageDataGenerator(
                featurewise_center=False,  # set input mean to 0 over the dataset
                samplewise_center=False,  # set each sample mean to 0
                featurewise_std_normalization=False,  # divide inputs by std of the dataset
                samplewise_std_normalization=False,  # divide each input by its std
                zca_whitening=False,  # apply ZCA whitening
                rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
                horizontal_flip=True,  # randomly flip images
                vertical_flip=False)  # randomly flip images

            # Compute quantities required for feature-wise normalization
            # (std, mean, and principal components if ZCA whitening is applied).
            datagen.fit(x_train)

            # Fit the model on the batches generated by datagen.flow().
            model.fit_generator(datagen.flow(x_train, y_train,
                                             batch_size=batch_size),
                                steps_per_epoch=steps_per_epoch,
                                epochs=epochs,
                                verbose=True,
                                validation_data=(x_test, y_test),
                                workers=4)

        # Save model and weights
        # model_name = 'keras_cifar10_trained_model.h5'
        # save_dir = os.path.join(os.getcwd(), 'saved_models')
        # if not os.path.isdir(save_dir):
        #     os.makedirs(save_dir)
        # model_path = os.path.join(save_dir, model_name)
        # model.save(model_path)
        # print('Saved trained model at %s ' % model_path)

        # Score trained model.
        score = model.evaluate(x_test,
                               y_test,
                               batch_size=batch_size,
                               verbose=True,
                               sample_weight=None)
        print('Test loss:', score[0])
        print('Test accuracy:', score[1])
        batch_sizes.append(x)
        scores.append(score[1])

    stop_time = dt.datetime.now()
    print("Elapsed time = ", (stop_time - start_time).total_seconds(), "s.")

    plt.xlabel('Batch Sizes')
    plt.ylabel('Accuracy')
    plt.title('Test Accuracy vs. Batch Size')
    plt.plot(batch_sizes,scores,'ro')

    save_dir = os.path.join(os.getcwd(), 'saved_imgs')
    if not os.path.isdir(save_dir):
        os.makedirs(save_dir)
    img_path = os.path.join(save_dir, 'acc_v_batch.png')
    plt.savefig(img_path)
    #plt.show()

    pass

if __name__ == '__main__':
    main()
